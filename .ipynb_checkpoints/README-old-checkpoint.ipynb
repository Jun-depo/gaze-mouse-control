{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Pointer Controller\n",
    "\n",
    "The goal of project is to builds an application that use human gaze to control mouse movement. In order to achieve that, one needs use 4 prebuilt AI models as listed below:\n",
    "\n",
    "* [face-detection-model](https://docs.openvinotoolkit.org/latest/_models_intel_face_detection_adas_binary_0001_description_face_detection_adas_binary_0001.html)\n",
    "* [landmarks-detection-model](https://docs.openvinotoolkit.org/latest/_models_intel_landmarks_regression_retail_0009_description_landmarks_regression_retail_0009.html)\n",
    "* [head-pose-estimation-model](https://docs.openvinotoolkit.org/latest/_models_intel_head_pose_estimation_adas_0001_description_head_pose_estimation_adas_0001.html)\n",
    "* [gaze-estimation-model](https://docs.openvinotoolkit.org/latest/_models_intel_gaze_estimation_adas_0002_description_gaze_estimation_adas_0002.html)\n",
    "\n",
    "The workflow of the project is shown below. \n",
    "<img src=\"images/workflow.png\" style=\"width: 450px; height: 370px;\">\n",
    "\n",
    "I use Intel Opevino libray inference engine to build this project, which includes the following source code files:\n",
    "* input_feeder.py (to load  input)\n",
    "* face_detection.py  (to process images of a video as inputs to get human face detetion boxes and face crops)  \n",
    "* facial_landmarks_detection.py (to process face_detection crops as inputs to obtain 5 face landmarks including left and right eye landmarks, and then to get left and right eye crops)\n",
    "* head_pose_estimation.py (The model file to process face_detection crops as inputs to get yaw, pitch and roll angles)\n",
    "* gaze_estimation.py (The model file to process left/right eye crops, and yaw/pitch/roll angles to get gaze vectors)\n",
    "* mouse_controller.py (to process gaze vectors and then  move the mouse cursor) \n",
    "* main.py (to use all others file and put things together to build the app)  \n",
    "\n",
    "\n",
    "## Project Set Up and Installation\n",
    "The project directory structure is demonstrated as following. \n",
    "\n",
    "<img src=\"images/directory_tree.png\" style=\"width: 400px; height: 800px;\">\n",
    "\n",
    "As you can see, I have two sets of files for running sychronously and asychronously inference, such as main.py, main_async.py, face_detection.py, face_detection_async.py, etc. \n",
    "\n",
    "The four pre-trained AI models were download through Openvino using the command line below for landmark_detection_model as an example.  \n",
    "* /opt/intel/openvino/deployment_tools/open_model_zoo/tools/downloader/downloader.py --name landmarks-regression-retail-0009  -o /home/jun-lp/Computer_Pointer_Controller_proj/app/\n",
    "\n",
    "* The other models can be downloaded similarly.    \n",
    "\n",
    "* all the models are saved under \"intel\" folder containing FP16, FP16-INT8 (for INT8) and FP32 precisions for the models. \n",
    "\n",
    "### To install dependencies, \n",
    "Build and activate virtual environment and install  dependencies.\n",
    "* pip install virtualenv\n",
    "* cd ~/Computer_Pointer_Controller_proj\n",
    "* virtualenv -p /usr/bin/python3.7 pointer_venv\n",
    "* source pointer_venv/bin/activate\n",
    "* pip3 install -r requirements.txt\n",
    "* sudo apt-get install python3-tk python3-dev (for MouseInfo)\n",
    "\n",
    "## Demo\n",
    "\n",
    "1. activate virtual environment: source pointer_venv/bin/activate\n",
    "\n",
    "2.  activate openvino: source /opt/intel/openvino/bin/setupvars.sh\n",
    "\n",
    "3. Run the app: python3 main.py --precision FP32 (an example for CPU and precision FP32 on the recorded video)\n",
    "\n",
    "\n",
    "## Documentation\n",
    "\n",
    "The application takes the following arguments. \n",
    "* --precision: model precision used for the inference engine (FP32, FP16, FP16-INT8). \n",
    "* --device (default='CPU') :  inference engine device. \n",
    "* --input_type (default=\"video\"):  for input data type (video, cam, etc). \n",
    "* --output_path is used for out_video file (for tracking bounding boxes, eye_landmarks).  \n",
    "* --threshold (default=0.6) is used by face_detection_model as the threshold for selecting face_detection boxes.  \n",
    "\n",
    "The precision argument was used as shown in the picture below:  \n",
    "\n",
    "<img src=\"images/precision_arg.png\" style=\"width: 1000px; height: 120px;\">\n",
    "\n",
    "The following are examples of running the app with command line argument \n",
    "* CPU and recorded video: **python3 main.py --precision FP32**\n",
    "\n",
    "* GPU and recorded video: **python3 main.py --precision FP16 --device GPU**\n",
    "\n",
    "* CPU and webcam: **python3 main.py --precision FP32 --input_type cam**\n",
    "\n",
    "\n",
    "## Benchmarks\n",
    "I used model loading time, average input time per frame, average inference time per frame, average mouse controlling time per frame (final_output). \n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "* MouseController setting at (precision=\"high\", speed='fast') appears to give best controlling result.  I measured the performance benchmarks with that setting. Changing precision=\"medium\" didn't affect mouse controller running time (see the last test).  \n",
    "\n",
    "CPU, FP32 and MouseController(precision=\"high\", speed='fast'): \n",
    "* model loading time:  0.6453\n",
    "* img counts: 59 (batch_size: 10)\n",
    "* average input time per frame:  0.0575\n",
    "* average inference time per frame:  0.0201\n",
    "* average mouse controlling time per frame:  1.1740\n",
    "\n",
    "CPU, FP16 and MouseController(precision=\"high\", speed='fast'): \n",
    "* model loading time:  0.6771\n",
    "* img counts: 59 (batch_size: 10)\n",
    "* average input time per frame:  0.0553\n",
    "* average inference time per frame:  0.0197\n",
    "* average mouse controlling time per frame:  1.1730\n",
    "\n",
    "CPU, INT8 and MouseController(precision=\"high\", speed='fast'): \n",
    "* model loading time: 1.4655\n",
    "* img counts: 59 (batch_size: 10)\n",
    "* average input time per frame: 0.0545 \n",
    "* average inference time per frame:  0.0183\n",
    "* average mouse controlling time per frame: 1.1723\n",
    "\n",
    "\n",
    "HETERO:GPU,CPU, FP16 and and MouseController(precision=\"high\", speed='fast'):\n",
    "(there are layers not supported by GPU. I have to use GPU and CPU)\n",
    "* model loading time: 71.6816\n",
    "* img counts: 59 (batch_size: 10)\n",
    "* average input time per frame: 0.0547 \n",
    "* average inference time per frame:  0.0251\n",
    "* average mouse controlling time per frame: 1.1707\n",
    "* CPU, FP32 and MouseController(precision=\"medium\", speed='fast'): \n",
    "\n",
    "CPU, FP32 and MouseController(precision=\"medium\", speed='fast'):\n",
    "* model loading time: 0.6301\n",
    "* img counts: 59 (batch_size: 10)\n",
    "* average input time per frame: 0.0557 \n",
    "* average inference time per frame:  0.0199\n",
    "* average mouse controlling time per frame: 1.1728\n",
    "\n",
    "Overall, controlling mouse cursor movement consumes most of run time. The plot\n",
    "below shows different step run time under \"CPU, FP32 and MouseController(precision=\"high\", speed='fast') running consition. Unlike other steps, model loading time is one time event for each run.\n",
    "\n",
    "<img src=\"images/app_running_time_steps.png\" style=\"width: 500px; height: 300px;\">\n",
    "\n",
    "The HETERO:GPU,CPU shows much longer model loading time.\n",
    "\n",
    "<img src=\"images/model_loading.png\" style=\"width: 500px; height: 300px;\">\n",
    "\n",
    "The HETERO:GPU,CPU has some what longer inference time (sychronous). Others are similar\n",
    "\n",
    "<img src=\"images/inference_time.png\" style=\"width: 500px; height: 300px;\">\n",
    "\n",
    "Different running conditions have very similar input time also.\n",
    "\n",
    "<img src=\"images/input_time.png\" style=\"width: 500px; height: 300px;\">\n",
    "\n",
    "## Stand Out Suggestions\n",
    "This is where you can provide information about the stand out suggestions that you have attempted.\n",
    "\n",
    "The precisions of the model weights don't seem to affect overall running time of this app. The process of controlling and moving the mouse consumes the most processing time. In order to enhance system performance, it would be worthwhile to search / develop for approaches that can improve mouse controller running time.   \n",
    "\n",
    "### Async Inference\n",
    "\n",
    "#### Asynchronous Inference\n",
    "python3 main_async.py --precision FP32\n",
    "model loading time: 0.6236\n",
    "img counts: 59\n",
    "average input time per frame: 0.0574 \n",
    "average inference time per frame:  0.0205\n",
    "average mouse controlling time per frame: 1.1754\n",
    "totoal app running time:  76.26487827301025\n",
    "\n",
    "#### synchronous Inference\n",
    "python3 main.py --precision FP32\n",
    "model loading time: 0.7209\n",
    "img counts: 59\n",
    "average input time per frame: 0.0579 \n",
    "average inference time per frame:  0.0200\n",
    "average mouse controlling time per frame: 1.1714\n",
    "totoal app running time:  76.11482954025269\n",
    "\n",
    "The results are very similar between asynchronous inference vs synchronous inference under my local environment. Runn time is largely determined by the controlling of mouse movement. \n",
    "\n",
    "### Edge Cases\n",
    "\n",
    "When I used my laptop webcam live video as input, the program often crashes due to unable to get face_detection, that face_crop image to be empty as show in the follow screenshot.  The laptop webcam has low signal/noise ratio. Under the indoor light, the video images are not good enough for consistent face_detection with the model.  Increasing light caused less program crash.  I also used a USB Logitech Webcam with much better signal/noise ratio completely corrected the crashing problem. \n",
    "\n",
    "<img src=\"images/edge1.png\" style=\"width: 450px; height: 100px;\">\n",
    "\n",
    "The selection two eye boxes was optimized for the recorded video.  It was not always the best choice for the webcams.  The box selection could be potentially modified by using fractions of width and height of face_detection_box as the size of eye boxes instead of fixed size, that can be more adaptable for different video sources.  Haven't  done experiments to select good parameters for the approach. But, it is definitely worth trying.        \n",
    "\n",
    "I haven't tested multiple people in the frame situation. Under such situation, this app should detect several face detection boxes. I would pick one face_detection box as the person of interest whose gaze would be used to guide mouse cursor movement.  Assuming there isn't too fast movement of people, face_detection_boxes of next image will be compare to the initial person of interest face_detection_box for the overlap by using intersection/union (IOU)) as the measurement. Only select the box, which has the highest IOU, to be fed into the subsequent models. Reset this selected face_detection box as the box for the next step overlapping selection.  I think this approach should resolved  multiple people in the frame situation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1-14",
   "language": "python",
   "name": "tf1-14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
