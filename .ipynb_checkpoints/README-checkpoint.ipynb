{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Pointer Controller\n",
    "\n",
    "The goal of project is to build an application that use human gaze to control mouse movement. In order to achieve that, I need to use 4 prebuilt AI models as listed below:\n",
    "\n",
    "* [face-detection-model](https://docs.openvinotoolkit.org/latest/_models_intel_face_detection_adas_binary_0001_description_face_detection_adas_binary_0001.html)\n",
    "* [landmarks-detection-model](https://docs.openvinotoolkit.org/latest/_models_intel_landmarks_regression_retail_0009_description_landmarks_regression_retail_0009.html)\n",
    "* [head-pose-estimation-model](https://docs.openvinotoolkit.org/latest/_models_intel_head_pose_estimation_adas_0001_description_head_pose_estimation_adas_0001.html)\n",
    "* [gaze-estimation-model](https://docs.openvinotoolkit.org/latest/_models_intel_gaze_estimation_adas_0002_description_gaze_estimation_adas_0002.html)\n",
    "\n",
    "The workflow of the project is shown below. \n",
    "<img src=\"images/workflow.png\" style=\"width: 420px; height: 320px;\">\n",
    "\n",
    "I use Intel Opevino libray inference engine to build this project, which includes the following source code files:\n",
    "* input_feeder.py (to load input)\n",
    "* face_detection.py  (process images of a video as inputs to get human face detetion boxes and face crops)  \n",
    "* facial_landmarks_detection.py (process face_detection crops as inputs to obtain 5 face landmarks including left and right eye landmarks, and then to get left and right eye crops)\n",
    "* head_pose_estimation.py (process face_detection crops as inputs to get yaw, pitch and roll angles)\n",
    "* gaze_estimation.py (process left/right eye crops, and yaw/pitch/roll angles to get gaze vectors)\n",
    "* mouse_controller.py (process gaze vectors and then move the mouse cursor) \n",
    "* main.py (use all other code files and put things together to build the app) \n",
    "\n",
    "Also, the following files are used for asynchronous inference.\n",
    "* face_detection_async.py\n",
    "* facial_landmarks_detection_async.py\n",
    "* head_pose_estimation_async.py\n",
    "* gaze_estimation_async.py\n",
    "* main_async.py\n",
    "\n",
    "\n",
    "## Project Set Up and Installation\n",
    "The project directory structure is demonstrated as following. \n",
    "\n",
    "<img src=\"images/directory_tree.png\" style=\"width: 360px; height: 1000px;\">\n",
    "\n",
    "The four pre-trained AI models were download through Openvino using the command line below for landmark_detection_model as an example.  \n",
    "* /opt/intel/openvino/deployment_tools/open_model_zoo/tools/downloader/downloader.py --name landmarks-regression-retail-0009  -o /home/jun-lp/Computer_Pointer_Controller_proj/app/\n",
    "\n",
    "* The other models can be downloaded similarly.    \n",
    "\n",
    "* all the models are saved under \"intel\" folder containing FP16, FP16-INT8 (for INT8) and FP32 precisions for these models. \n",
    "\n",
    "### To install dependencies, \n",
    "Build and activate virtual environment and install  dependencies.\n",
    "* pip install virtualenv\n",
    "* cd ~/Computer_Pointer_Controller_proj\n",
    "* virtualenv -p /usr/bin/python3.7 pointer_venv\n",
    "* source pointer_venv/bin/activate\n",
    "* pip3 install -r requirements.txt\n",
    "* sudo apt-get install python3-tk python3-dev (for MouseInfo)\n",
    "\n",
    "## Demo\n",
    "\n",
    "1. activate virtual environment: source pointer_venv/bin/activate\n",
    "\n",
    "2. activate openvino: source /opt/intel/openvino/bin/setupvars.sh\n",
    "\n",
    "3. Run the app: python3 main.py --precision FP32 (an example for CPU and precision FP32 on the recorded video)\n",
    "\n",
    "\n",
    "## Documentation\n",
    "\n",
    "The application takes the following arguments. \n",
    "* --precision: model precision used for the inference engine (FP32, FP16, FP16-INT8). \n",
    "* --device (default='CPU') :  inference engine device. \n",
    "* --input_type (default=\"video\"):  for input data type (video, cam, etc). \n",
    "* --output_path is used for out_video file (for tracking bounding boxes, eye_landmarks).  \n",
    "* --threshold (default=0.6) is used by face_detection_model as the threshold for selecting face_detection boxes. \n",
    "* --get_perf_counts' (default=False). When True, Runtime performance benchmarks are printed at terminal console.   \n",
    "* --mouse_with_video' (default=False). When True, input source video is displayed on the screen.\n",
    "* --display_boxes (default=False). When True, face_detection_box, eye_landmarks and eye_boxes are displayed on the  video.  \n",
    "\n",
    "The precision argument was used as shown in the picture below:  \n",
    "\n",
    "<img src=\"images/args_precision.png\" style=\"width: 800px; height: 120px;\">\n",
    "\n",
    "The following are examples of running the app with command line argument \n",
    "* CPU and recorded video: **python3 main.py --precision FP32**\n",
    "\n",
    "* GPU and recorded video: **python3 main.py --precision FP16 --device GPU**\n",
    "\n",
    "* CPU and webcam: **python3 main.py --precision FP32 --input_type cam**\n",
    "\n",
    "## Benchmarks\n",
    "I used model loading time, average input time per frame, average inference time per frame, average mouse controlling time per frame (final_output). \n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "* MouseController setting at (precision=\"high\", speed='fast') appears to give best controlling result.  I measured the performance benchmarks with that setting. Changing precision=\"medium\" didn't affect mouse controller running time. I list two sets of running results and plots.\n",
    "\n",
    "CPU, FP32 and MouseController(precision=\"high\", speed='fast'): \n",
    "* model loading time:  0.6453\n",
    "* img counts: 59 (batch_size: 10)\n",
    "* average input time per frame:  0.0575\n",
    "* average inference time per frame:  0.0201\n",
    "* average mouse controlling time per frame:  1.1740\n",
    "\n",
    "CPU, FP32 and MouseController(precision=\"medium\", speed='fast'):\n",
    "* model loading time: 0.6301\n",
    "* img counts: 59 (batch_size: 10)\n",
    "* average input time per frame: 0.0557 \n",
    "* average inference time per frame:  0.0199\n",
    "* average mouse controlling time per frame: 1.1728\n",
    "\n",
    "Overall, controlling mouse cursor movement consumes most of run time, although I am not sure this is due to my particular computer mouse controller or general slow runtime of mouse controller .  The plot\n",
    "below shows different steps' run time under \"CPU, FP32 and MouseController(precision=\"high\", speed='fast') condition. Unlike other steps, model loading time is one time event for each run.\n",
    "\n",
    "<img src=\"images/app_running_time_steps.png\" style=\"width: 400px; height: 260px;\">\n",
    "\n",
    "The HETERO:GPU,CPU shows much longer model loading time.\n",
    "\n",
    "<img src=\"images/model_loading.png\" style=\"width: 400px; height: 260px;\">\n",
    "\n",
    "The HETERO:GPU,CPU has some what longer inference time (sychronous). Others are similar\n",
    "\n",
    "<img src=\"images/inference_time.png\" style=\"width: 400px; height: 260px;\">\n",
    "\n",
    "Different running conditions have very similar model input time also.\n",
    "\n",
    "<img src=\"images/input_time.png\" style=\"width: 400px; height: 260px;\">\n",
    "\n",
    "## Stand Out Suggestions\n",
    "* I implement some of suggestions that my code to run inference on webcam with \"--input_type cam\". Also, include demo1_webcam.mp4 I recorded from the screen. \n",
    "* I also add \"--get_perf_conuts\" argument and \"--mouse_with_video argument\". I set the default as \"False\" for both.  They can be switched to \"True\" to display video and print performance benchmarks. \n",
    "* I add \"--display_boxes\" argument to select whether to display detection boxes and facial landmarks or not.                      \n",
    "\n",
    "### Async Inference\n",
    "\n",
    "I compared the results of running asynchronous inference and synchronous inference as shown below.\n",
    "\n",
    "#### Asynchronous Inference\n",
    "* python3 main_async.py --precision FP32\n",
    "* model loading time: 0.6236\n",
    "* average input time per frame: 0.0574 \n",
    "* average inference time per frame:  0.0205\n",
    "* average mouse controlling time per frame: 1.1754\n",
    "* totoal app running time:  76.26487827301025\n",
    "\n",
    "#### synchronous Inference\n",
    "* python3 main.py --precision FP32\n",
    "* model loading time: 0.7209\n",
    "* img counts: 59\n",
    "* average input time per frame: 0.0579 \n",
    "* average inference time per frame:  0.0200\n",
    "* average mouse controlling time per frame: 1.1714\n",
    "* totoal app running time:  76.11482954025269\n",
    "\n",
    "The benchmarks are very similar between asynchronous inference vs synchronous inference under my local environment. Run time is still largely determined by the controlling of mouse movement. \n",
    "\n",
    "### Edge Cases\n",
    "\n",
    "When I used my laptop webcam live video as input, the program often crashes due to unable to get face_detection, that face_crop image to be empty as show in the follow screenshot.  The laptop webcam has low signal/noise ratio. Under the indoor light, the video images are not good enough for consistent face_detection with the model.  Increasing light caused less program crash.  I also used a USB Logitech Webcam with much better signal/noise ratio completely corrected the crashing problem. \n",
    "\n",
    "<img src=\"images/edge1.png\" style=\"width: 450px; height: 100px;\">\n",
    "\n",
    "The selection two eye boxes was optimized for the recorded video.  It was not always the best choice for the webcams.  The box selection could be potentially modified by using fractions of width and height of face_detection_box as the size of eye boxes instead of fixed size, that can be more adaptable for different video sources.  Haven't done experiments to select good parameters for the approach. But, it is definitely worth trying.        \n",
    "\n",
    "I haven't tested multiple people in the frame situation. Under such situation, this app should detect several face detection boxes. I would pick one face_detection box as the person of interest whose gaze would be used to guide mouse cursor movement.  Assuming there isn't too fast movement of people, face_detection_boxes of next image will be compare to the initial person of interest face_detection_box for the overlap by using intersection/union (IOU) as the measurement. Only select the box, which has the highest IOU, to be fed into the subsequent models. Reset this selected face_detection box as the box for the next step IOU overlapping selection.  I think this approach should resolve multiple people in the frame situation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1-14",
   "language": "python",
   "name": "tf1-14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
